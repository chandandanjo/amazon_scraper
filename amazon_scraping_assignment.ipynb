{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_scraping_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaShA30rZ6PxwMcamIsEBu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandandanjo/amazon_scraper/blob/main/amazon_scraping_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests-html regex"
      ],
      "metadata": {
        "id": "I3vRhg5pNPGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fs6om5cnblgn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from requests_html import HTMLSession\n",
        "import time\n",
        "import regex\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class amazonScraper:\n",
        "    def __init__(self, csv_path):\n",
        "        self.csv_path = csv_path\n",
        "        self.output_list = []\n",
        "\n",
        "    def csv_extractor(self):\n",
        "        # Function to extract ASIN and Country-code from CSV file.\n",
        "        dataset = pd.read_csv(self.csv_path)\n",
        "        df = pd.DataFrame(dataset)\n",
        "        country = list(df['country'])\n",
        "        asin = list(df['Asin'])\n",
        "        return country, asin\n",
        "\n",
        "    def scraper(self, country, asin):\n",
        "        s = HTMLSession()\n",
        "        url = f'https://www.amazon.{country}/dp/{asin}'\n",
        "\n",
        "        # initialising output dictionary with null values.\n",
        "        output = {'Product Title': 'NA', 'Product Image URL': 'NA', 'Price of the Product': 'NA', 'Product Details': 'NA', 'URL' : url}\n",
        "        resp = s.get(url)\n",
        "        if resp.status_code == 200:\n",
        "            try:\n",
        "                product_details_ = resp.html.xpath(\"//ul[contains(@class, 'detail')] | //table[@role='presentation']\", first=True)\n",
        "                product_details = {}\n",
        "                try:\n",
        "                    df = [i.split(':') for i in product_details_.text.split('\\n')]\n",
        "                    for ls in df:\n",
        "                        product_details[''.join(str(i) for i in str(ls[0]) if ord(i) < 128).strip()] = ''.join(i for i in str(ls[1]) if ord(i) < 128).strip()\n",
        "                except:\n",
        "                    df_ = product_details_.text.split('\\n')\n",
        "                    index = 0\n",
        "                    while index in range(len(df_)):\n",
        "                        product_details[(''.join(str(i) for i in (df_[index]) if ord(i) < 128)).strip()] = (''.join(str(i) for i in (df_[index+1]) if ord(i) < 128)).strip()\n",
        "                        index += 2\n",
        "                # Single product output file.        \n",
        "                output = {\n",
        "                    'Product Title': resp.html.xpath(\"//span[@id='productTitle']\", first=True).text,\n",
        "                    'Product Image URL': (resp.html.xpath(\"//img[contains(@class,'stretch-horizontal')] | //img[contains(@class,'stretch-vertical')]\", first=True)).attrs['src'],\n",
        "                    'Price of the Product': ''.join(i for i in str(regex.findall(r\"\\d+[.|,]\\d+[^\\p{Sc}|$\\p{Sc}]\", resp.html.xpath(\"//span[contains(text(),'â‚¬')]\", first=True).text)[0]).replace(',','.') if i.isnumeric() or i == '.'),\n",
        "                    'Product Details': product_details,\n",
        "                    'URL' : url\n",
        "                }\n",
        "                # Appending dictionary to main list.\n",
        "                self.output_list.append(output)\n",
        "            except Exception as e:\n",
        "                print('*'*50)\n",
        "                print(e)\n",
        "                print(resp.html)\n",
        "                print('*'*50)\n",
        "        elif resp.status_code == 404:\n",
        "            print('*'*50)\n",
        "            print(f'{url} : 404 Not found.')\n",
        "            print('*'*50)\n",
        "        else:\n",
        "            print('*'*50)\n",
        "            print(f'{url} : {resp.status_code} Error code.')\n",
        "            print('*'*50)\n",
        "\n",
        "    def main(self):\n",
        "        countries, asins = self.csv_extractor()\n",
        "        for country, asin in zip(countries, asins):\n",
        "            if len(self.output_list) < 101:\n",
        "                self.scraper(country, asin)\n",
        "                # Implicitly sleeping for 30 seconds to avoid IP blocking / Reducing requests per second load to avoid detection.\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                break\n",
        "        # Converting list of dictionaries to JSON.\n",
        "        data = json.dumps(self.output_list, indent=2)\n",
        "        with open(\"mydata.json\", \"w\") as final:\n",
        "            final.write(data)\n",
        "        \n",
        "        return self.output_list # Returning list of dictionaries.\n",
        "        "
      ],
      "metadata": {
        "id": "rCBt6xIbXO4i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'amzn.csv'\n",
        "obj = amazonScraper(PATH)\n",
        "\n",
        "start_time = time.time()\n",
        "obj.main()\n",
        "print(\" %s seconds\" % timedelta(seconds = (time.time() - start_time)))"
      ],
      "metadata": {
        "id": "XTHExdc8d_dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}